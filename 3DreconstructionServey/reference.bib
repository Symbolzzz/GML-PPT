@inproceedings{barronMipNeRF360Unbounded2022,
  title = {Mip-{{NeRF}} 360: {{Unbounded Anti-Aliased Neural Radiance Fields}}},
  shorttitle = {Mip-{{NeRF}} 360},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P. and Hedman, Peter},
  year = {2022},
  pages = {5460--5469},
  publisher = {{IEEE}},
  location = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.00539},
  url = {https://ieeexplore.ieee.org/document/9878829/},
  urldate = {2023-11-12},
  abstract = {Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on “unbounded” scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub “mip-NeRF 360” as we target scenes in which the camera rotates 360 degrees around a point, reduces meansquared error by 57\% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  langid = {english},
  keywords = {ObsCite},
  file = {/Users/xuanenyun/Zotero/storage/YHZ6SUY7/Barron 等 - 2022 - Mip-NeRF 360 Unbounded Anti-Aliased Neural Radian.pdf}
}

@article{barronMipNeRFMultiscaleRepresentation,
  title = {Mip-{{NeRF}}: {{A Multiscale Representation}} for {{Anti-Aliasing Neural Radiance Fields Supplemental Material}}},
  author = {Barron, Jonathan T and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P},
  year = {2021},
  langid = {english},
  keywords = {ObsCite},
  file = {/Users/xuanenyun/Zotero/storage/JQJR34NT/Barron 等 - Mip-NeRF A Multiscale Representation for Anti-Ali.pdf}
}

@inproceedings{barronMipNeRFMultiscaleRepresentation2021,
  title = {Mip-{{NeRF}}: {{A Multiscale Representation}} for {{Anti-Aliasing Neural Radiance Fields}}},
  shorttitle = {Mip-{{NeRF}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
  year = {2021},
  pages = {5835--5844},
  publisher = {{IEEE}},
  location = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.00580},
  url = {https://ieeexplore.ieee.org/document/9710056/},
  urldate = {2023-12-04},
  abstract = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call “mip-NeRF” (a` la “mipmap”), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF’s ability to represent fine details, while also being 7\% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17\% on the dataset presented with NeRF and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22× faster.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  langid = {english},
  keywords = {ObsCite},
  file = {/Users/xuanenyun/Zotero/storage/29XTCN5Q/Barron_Mip-NeRF_A_Multiscale_Representation_for_Anti-Aliasing_Neural_Radiance_Fields_ICCV_2021_paper.pdf}
}

@incollection{caoLearningReconstructHighQuality2018,
  title = {Learning to {{Reconstruct High-Quality 3D Shapes}} with {{Cascaded Fully Convolutional Networks}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Cao, Yan-Pei and Liu, Zheng-Ning and Kuang, Zheng-Fei and Kobbelt, Leif and Hu, Shi-Min},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11213},
  pages = {626--643},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01240-3_38},
  url = {https://link.springer.com/10.1007/978-3-030-01240-3_38},
  urldate = {2024-01-26},
  isbn = {978-3-030-01239-7 978-3-030-01240-3},
  langid = {english}
}

@online{changShapeNetInformationRich3D2015,
  title = {{{ShapeNet}}: {{An Information-Rich 3D Model Repository}}},
  shorttitle = {{{ShapeNet}}},
  author = {Chang, Angel X. and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},
  year = {2015},
  eprint = {1512.03012},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1512.03012},
  urldate = {2024-01-25},
  abstract = {We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Robotics}
}

@inproceedings{chenMVSNeRFFastGeneralizable2021,
  title = {{{MVSNeRF}}: {{Fast Generalizable Radiance Field Reconstruction}} from {{Multi-View Stereo}}},
  shorttitle = {{{MVSNeRF}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Chen, Anpei and Xu, Zexiang and Zhao, Fuqiang and Zhang, Xiaoshuai and Xiang, Fanbo and Yu, Jingyi and Su, Hao},
  year = {2021},
  pages = {14104--14113},
  publisher = {{IEEE}},
  location = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.01386},
  url = {https://ieeexplore.ieee.org/document/9711430/},
  urldate = {2024-01-28},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  file = {/Users/xuanenyun/Zotero/storage/SBH6PGAM/Chen 等 - 2021 - MVSNeRF Fast Generalizable Radiance Field Reconst.pdf}
}

@online{chenSurvey3DGaussian2024,
  title = {A {{Survey}} on {{3D Gaussian Splatting}}},
  author = {Chen, Guikun and Wang, Wenguan},
  year = {2024},
  eprint = {2401.03890},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2401.03890},
  urldate = {2024-01-16},
  abstract = {3D Gaussian splatting (3D GS) has recently emerged as a transformative technique in the explicit radiance field and computer graphics landscape. This innovative approach, characterized by the utilization of millions of 3D Gaussians, represents a significant departure from the neural radiance field (NeRF) methodologies, which predominantly use implicit, coordinate-based models to map spatial coordinates to pixel values. 3D GS, with its explicit scene representations and differentiable rendering algorithms, not only promises real-time rendering capabilities but also introduces unprecedented levels of control and editability. This positions 3D GS as a potential game-changer for the next generation of 3D reconstruction and representation. In the present paper, we provide the first systematic overview of the recent developments and critical contributions in the domain of 3D GS. We begin with a detailed exploration of the underlying principles and the driving forces behind the advent of 3D GS, setting the stage for understanding its significance. A focal point of our discussion is the practical applicability of 3D GS. By facilitating real-time performance, 3D GS opens up a plethora of applications, ranging from virtual reality to interactive media and beyond. This is complemented by a comparative analysis of leading 3D GS models, evaluated across various benchmark tasks to highlight their performance and practical utility. The survey concludes by identifying current challenges and suggesting potential avenues for future research in this domain. Through this survey, we aim to provide a valuable resource for both newcomers and seasoned researchers, fostering further exploration and advancement in applicable and explicit radiance field representation.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Multimedia,survey},
  file = {/Users/xuanenyun/Zotero/storage/E3YBKX67/Chen 和 Wang - 2024 - A Survey on 3D Gaussian Splatting.pdf;/Users/xuanenyun/Zotero/storage/QHJKSYRJ/2401.html}
}

@inproceedings{dengDepthsupervisedNeRFFewer2022,
  title = {Depth-Supervised {{NeRF}}: {{Fewer Views}} and {{Faster Training}} for {{Free}}},
  shorttitle = {Depth-Supervised {{NeRF}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Deng, Kangle and Liu, Andrew and Zhu, Jun-Yan and Ramanan, Deva},
  year = {2022},
  pages = {12872--12881},
  publisher = {{IEEE}},
  location = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.01254},
  url = {https://ieeexplore.ieee.org/document/9880067/},
  urldate = {2023-11-12},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  langid = {english},
  keywords = {ObsCite},
  file = {/Users/xuanenyun/Zotero/storage/954J89EZ/Deng 等 - 2022 - Depth-supervised NeRF Fewer Views and Faster Trai.pdf;/Users/xuanenyun/Zotero/storage/UTZ9TSU5/扫描全能王 2023-12-27 17.28(1).pdf}
}

@article{eigenDepthMapPrediction,
  title = {Depth {{Map Prediction}} from a {{Single Image}} Using a {{Multi-Scale Deep Network}}},
  author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
  abstract = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.},
  langid = {english},
  file = {/Users/xuanenyun/Zotero/storage/E6QPHIPY/Eigen 等 - Depth Map Prediction from a Single Image using a M.pdf},
  year = {2014}
}

@inproceedings{elbananiNovelObjectViewpoint2020,
  title = {Novel {{Object Viewpoint Estimation Through Reconstruction Alignment}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {El Banani, Mohamed and Corso, Jason J. and Fouhey, David F.},
  year = {2020},
  pages = {3110--3119},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00318},
  url = {https://ieeexplore.ieee.org/document/9156616/},
  urldate = {2024-01-26},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  file = {/Users/xuanenyun/Zotero/storage/DTP2J54T/El Banani 等 - 2020 - Novel Object Viewpoint Estimation Through Reconstr.pdf}
}

@online{fanPointSetGeneration2016,
  title = {A {{Point Set Generation Network}} for {{3D Object Reconstruction}} from a {{Single Image}}},
  author = {Fan, Haoqiang and Su, Hao and Guibas, Leonidas},
  year = {2016},
  eprint = {1612.00603},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1612.00603},
  urldate = {2024-01-14},
  abstract = {Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations, and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output – point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-ofthe-art methods on single image based 3d reconstruction benchmarks; but it also shows strong performance for 3d shape completion and promising ability in making multiple plausible predictions.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {titleTranslation: 用于从单个图像重建 3D 对象的点集生成网络},
  file = {/Users/xuanenyun/Zotero/storage/M26VAM58/Fan 等 - 2016 - A Point Set Generation Network for 3D Object Recon.pdf}
}

@book{furukawaMultiviewStereoTutorial2015,
  title = {Multi-View Stereo: A Tutorial},
  shorttitle = {Multi-View Stereo},
  author = {Furukawa, Yasutaka and Hernández, Carlos},
  year = {2015},
  series = {Foundation and Trends in Computer Graphics and Vision},
  number = {9,1/2},
  publisher = {{Now}},
  location = {{Boston Delft}},
  isbn = {978-1-60198-836-2},
  langid = {english},
  pagetotal = {154},
  file = {/Users/xuanenyun/Zotero/storage/RC4STFKW/Furukawa 和 Hernández - 2015 - Multi-view stereo a tutorial.pdf}
}

@online{GaiNianLiJieChaoXiangSuFenGeZhiHu,
  title = {【概念理解】超像素分割 - 知乎},
  url = {https://zhuanlan.zhihu.com/p/356715155},
  urldate = {2024-01-24},
  file = {/Users/xuanenyun/Zotero/storage/9QKFCQWS/356715155.html}
}

@online{gaoNeRFNeuralRadiance2023,
  title = {{{NeRF}}: {{Neural Radiance Field}} in {{3D Vision}}, {{A Comprehensive Review}}},
  shorttitle = {{{NeRF}}},
  author = {Gao, Kyle and Gao, Yina and He, Hongjie and Lu, Dening and Xu, Linlin and Li, Jonathan},
  year = {2023},
  eprint = {2210.00379},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.00379},
  urldate = {2024-01-18},
  abstract = {Neural Radiance Field (NeRF) has recently become a significant development in the field of Computer Vision, allowing for implicit, neural network-based scene representation and novel view synthesis. NeRF models have found diverse applications in robotics, urban mapping, autonomous navigation, virtual reality/augmented reality, and more. Due to the growing popularity of NeRF and its expanding research area, we present a comprehensive survey of NeRF papers from the past two years. Our survey is organized into architecture and applicationbased taxonomies and provides an introduction to the theory of NeRF and its training via differentiable volume rendering. We also present a benchmark comparison of the performance and speed of key NeRF models. By creating this survey, we hope to introduce new researchers to NeRF, provide a helpful reference for influential works in this field, as well as motivate future research directions with our discussion section.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.4},
  file = {/Users/xuanenyun/Zotero/storage/4YWXVEXC/Gao 等 - 2023 - NeRF Neural Radiance Field in 3D Vision, A Compre.pdf}
}

@article{gengStructuredlight3DSurface2011,
  title = {Structured-Light {{3D}} Surface Imaging: A Tutorial},
  shorttitle = {Structured-Light {{3D}} Surface Imaging},
  author = {Geng, Jason},
  year = {2011},
  journaltitle = {Advances in Optics and Photonics},
  shortjournal = {Adv. Opt. Photon.},
  volume = {3},
  number = {2},
  pages = {128},
  issn = {1943-8206},
  doi = {10.1364/AOP.3.000128},
  url = {https://opg.optica.org/abstract.cfm?URI=aop-3-2-128},
  urldate = {2024-01-18},
  langid = {english},
  file = {/Users/xuanenyun/Zotero/storage/CFTQF9SD/Geng - 2011 - Structured-light 3D surface imaging a tutorial.pdf}
}

@online{groueixAtlasNetPapierMAch2018a,
  title = {{{AtlasNet}}: {{A Papier-M}}\textbackslash\^ach\textbackslash 'e {{Approach}} to {{Learning 3D Surface Generation}}},
  shorttitle = {{{AtlasNet}}},
  author = {Groueix, Thibault and Fisher, Matthew and Kim, Vladimir G. and Russell, Bryan C. and Aubry, Mathieu},
  year = {2018},
  eprint = {1802.05384},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.05384},
  urldate = {2024-01-26},
  abstract = {We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) autoencoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/xuanenyun/Zotero/storage/7GC89GXJ/Groueix 等 - 2018 - AtlasNet A Papier-M^ach'e Approach to Learning .pdf}
}

@article{guoDeepLearning3D2021,
  title = {Deep {{Learning}} for {{3D Point Clouds}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{3D Point Clouds}}},
  author = {Guo, Yulan and Wang, Hanyun and Hu, Qingyong and Liu, Hao and Liu, Li and Bennamoun, Mohammed},
  year = {2021},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {43},
  number = {12},
  pages = {4338--4364},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.3005434},
  url = {https://ieeexplore.ieee.org/document/9127813/},
  urldate = {2024-01-14},
  abstract = {Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.},
  langid = {english},
  keywords = {survey},
  file = {/Users/xuanenyun/Zotero/storage/8ALARFWG/Guo 等 - 2021 - Deep Learning for 3D Point Clouds A Survey.pdf}
}

@article{hamzahLiteratureSurveyStereo2016,
  title = {Literature {{Survey}} on {{Stereo Vision Disparity Map Algorithms}}},
  author = {Hamzah, Rostam Affendi and Ibrahim, Haidi},
  year = {2016},
  journaltitle = {Journal of Sensors},
  shortjournal = {Journal of Sensors},
  volume = {2016},
  pages = {1--23},
  issn = {1687-725X, 1687-7268},
  doi = {10.1155/2016/8742920},
  url = {http://www.hindawi.com/journals/js/2016/8742920/},
  urldate = {2024-01-17},
  abstract = {This paper presents a literature survey on existing disparity map algorithms. It focuses on four main stages of processing as proposed by Scharstein and Szeliski in a taxonomy and evaluation of dense two-frame stereo correspondence algorithms performed in 2002. To assist future researchers in developing their own stereo matching algorithms, a summary of the existing algorithms developed for every stage of processing is also provided. The survey also notes the implementation of previous software-based and hardware-based algorithms. Generally, the main processing module for a software-based implementation uses only a central processing unit. By contrast, a hardware-based implementation requires one or more additional processors for its processing module, such as graphical processing unit or a field programmable gate array. This literature survey also presents a method of qualitative measurement that is widely used by researchers in the area of stereo vision disparity mappings.},
  langid = {english},
  keywords = {survey},
  file = {/Users/xuanenyun/Zotero/storage/CXD4S8BP/Hamzah 和 Ibrahim - 2016 - Literature Survey on Stereo Vision Disparity Map A.pdf}
}

@article{jin3DReconstructionUsing2020,
  title = {{{3D}} Reconstruction Using Deep Learning: A Survey},
  shorttitle = {{{3D}} Reconstruction Using Deep Learning},
  author = {Jin, Yiwei and Jiang, Diqiong and Cai, Ming},
  year = {2020},
  journaltitle = {Communications in Information and Systems},
  volume = {20},
  number = {4},
  pages = {389--413},
  issn = {15267555, 21634548},
  doi = {10.4310/CIS.2020.v20.n4.a1},
  url = {https://www.intlpress.com/site/pub/pages/journals/items/cis/content/vols/0020/0004/a001/},
  urldate = {2024-01-17},
  langid = {english},
  keywords = {survey},
  file = {/Users/xuanenyun/Zotero/storage/GTZCCG5J/Jin 等 - 2020 - 3D reconstruction using deep learning a survey.pdf}
}

@inproceedings{katoNeural3DMesh2018,
  title = {Neural {{3D Mesh Renderer}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kato, Hiroharu and Ushiku, Yoshitaka and Harada, Tatsuya},
  year = {2018},
  pages = {3907--3916},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00411},
  url = {https://ieeexplore.ieee.org/document/8578509/},
  urldate = {2024-01-26},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9}
}

@online{kerbl3DGaussianSplatting2023,
  title = {{{3D Gaussian Splatting}} for {{Real-Time Radiance Field Rendering}}},
  author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkühler, Thomas and Drettakis, George},
  year = {2023},
  eprint = {2308.04079},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2308.04079},
  urldate = {2024-01-15},
  abstract = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time ({$>$}= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/xuanenyun/Zotero/storage/ALZIBKB6/Kerbl 等 - 2023 - 3D Gaussian Splatting for Real-Time Radiance Field.pdf}
}

@inproceedings{maInvitation3DVision2003,
  title = {An {{Invitation}} to 3-{{D Vision}}: {{From Images}} to {{Geometric Models}}},
  shorttitle = {An {{Invitation}} to 3-{{D Vision}}},
  author = {Ma, Y. and Soatto, Stefano and Koseck, Jana and Sastry, S.},
  year = {2003},
  url = {https://www.semanticscholar.org/paper/An-Invitation-to-3-D-Vision%3A-From-Images-to-Models-Ma-Soatto/05d63eb82662965127e9d98576a10810a5ee8e87#citing-papers},
  urldate = {2024-01-14},
  abstract = {This book introduces the geometry of 3-D vision, that is, the reconstruction of 3-D models of objects from a collection of 2-D images. It details the classic theory of two view geometry and shows that a more proper tool for studying the geometry of multiple views is the so-called rank consideration of the multiple view matrix. It also develops practical reconstruction algorithms and discusses possible extensions of the theory.}
}

@inproceedings{martin-bruallaNeRFWildNeural2021,
  title = {{{NeRF}} in the {{Wild}}: {{Neural Radiance Fields}} for {{Unconstrained Photo Collections}}},
  shorttitle = {{{NeRF}} in the {{Wild}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Martin-Brualla, Ricardo and Radwan, Noha and Sajjadi, Mehdi S. M. and Barron, Jonathan T. and Dosovitskiy, Alexey and Duckworth, Daniel},
  year = {2021},
  pages = {7206--7215},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00713},
  url = {https://ieeexplore.ieee.org/document/9578784/},
  urldate = {2024-01-28},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  file = {/Users/xuanenyun/Zotero/storage/3HX44IN8/Martin-Brualla 等 - 2021 - NeRF in the Wild Neural Radiance Fields for Uncon.pdf}
}

@inproceedings{meschederOccupancyNetworksLearning2019,
  title = {Occupancy {{Networks}}: {{Learning 3D Reconstruction}} in {{Function Space}}},
  shorttitle = {Occupancy {{Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
  year = {2019},
  pages = {4455--4465},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00459},
  url = {https://ieeexplore.ieee.org/document/8953655/},
  urldate = {2024-01-26},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  file = {/Users/xuanenyun/Zotero/storage/V62KSMGD/Mescheder 等 - 2019 - Occupancy Networks Learning 3D Reconstruction in .pdf}
}

@online{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  year = {2020},
  eprint = {2003.08934},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.08934},
  urldate = {2023-11-08},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, φ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,ObsCite},
  file = {/Users/xuanenyun/Zotero/storage/2JQI5CXI/Mildenhall 等 - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf}
}

@article{mullerInstantNeuralGraphics2022,
  title = {Instant {{Neural Graphics Primitives}} with a {{Multiresolution Hash Encoding}}},
  author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  year = {2022},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {41},
  number = {4},
  eprint = {2201.05989},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--15},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3528223.3530127},
  url = {http://arxiv.org/abs/2201.05989},
  urldate = {2023-11-12},
  abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of \$\{1920\textbackslash!\textbackslash times\textbackslash!1080\}\$.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,ObsCite},
  file = {/Users/xuanenyun/Zotero/storage/XEQXAKGR/Müller 等 - 2022 - Instant Neural Graphics Primitives with a Multires.pdf}
}

@article{MultipleViewGeometry2001,
  title = {Multiple {{View Geometry}} in {{Computer Vision}}},
  year = {2001},
  journaltitle = {Kybernetes},
  volume = {30},
  number = {9/10},
  pages = {1333--1341},
  publisher = {{Emerald Group Publishing Limited}},
  issn = {0368-492X},
  doi = {10.1108/k.2001.30.9_10.1333.2},
  url = {https://doi.org/10.1108/k.2001.30.9_10.1333.2},
  urldate = {2024-01-14}
}

@inproceedings{onizukaTetraTSDF3DHuman2020,
  title = {{{TetraTSDF}}: {{3D Human Reconstruction From}} a {{Single Image With}} a {{Tetrahedral Outer Shell}}},
  shorttitle = {{{TetraTSDF}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Onizuka, Hayato and Hayirci, Zehra and Thomas, Diego and Sugimoto, Akihiro and Uchiyama, Hideaki and Taniguchi, Rin-ichiro},
  year = {2020},
  pages = {6010--6019},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00605},
  url = {https://ieeexplore.ieee.org/document/9157455/},
  urldate = {2024-01-25},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5}
}

@inproceedings{panResidualMeshNetLearning2018,
  title = {Residual {{MeshNet}}: {{Learning}} to {{Deform Meshes}} for {{Single-View 3D Reconstruction}}},
  shorttitle = {Residual {{MeshNet}}},
  booktitle = {2018 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Pan, Junyi and Li, Jun and Han, Xiaoguang and Jia, Kui},
  year = {2018},
  pages = {719--727},
  publisher = {{IEEE}},
  location = {{Verona}},
  doi = {10.1109/3DV.2018.00087},
  url = {https://ieeexplore.ieee.org/document/8491025/},
  urldate = {2024-01-26},
  eventtitle = {2018 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  isbn = {978-1-5386-8425-2},
  file = {/Users/xuanenyun/Zotero/storage/SYJY9QR5/Pan 等 - 2018 - Residual MeshNet Learning to Deform Meshes for Si.pdf}
}

@online{SanWeiChongJianSuanFaZongShuChuanTongShenDuXueXiFangShi,
  title = {三维重建算法综述|传统+深度学习方式 - 知乎},
  url = {https://zhuanlan.zhihu.com/p/108422696},
  urldate = {2024-01-14},
  file = {/Users/xuanenyun/Zotero/storage/HDV4WENN/108422696.html}
}

@article{sarmahSurveyMethodsPrinciples2023,
  title = {Survey of Methods and Principles in Three-Dimensional Reconstruction from Two-Dimensional Medical Images},
  author = {Sarmah, Mriganka and Neelima, Arambam and Singh, Heisnam Rohen},
  year = {2023},
  journaltitle = {Visual Computing for Industry, Biomedicine, and Art},
  shortjournal = {Vis. Comput. Ind. Biomed. Art},
  volume = {6},
  number = {1},
  pages = {15},
  issn = {2524-4442},
  doi = {10.1186/s42492-023-00142-7},
  url = {https://vciba.springeropen.com/articles/10.1186/s42492-023-00142-7},
  urldate = {2024-01-17},
  abstract = {Three-dimensional (3D) reconstruction of human organs has gained attention in recent years due to advances in the Internet and graphics processing units. In the coming years, most patient care will shift toward this new paradigm. However, development of fast and accurate 3D models from medical images or a set of medical scans remains a daunting task due to the number of pre-processing steps involved, most of which are dependent on human expertise. In this review, a survey of pre-processing steps was conducted, and reconstruction techniques for several organs in medical diagnosis were studied. Various methods and principles related to 3D reconstruction were highlighted. The usefulness of 3D reconstruction of organs in medical diagnosis was also highlighted.},
  langid = {english},
  keywords = {survey},
  file = {/Users/xuanenyun/Zotero/storage/WEIC5DHU/Sarmah 等 - 2023 - Survey of methods and principles in three-dimensio.pdf}
}

@inproceedings{schonbergerStructurefromMotionRevisited2016,
  title = {Structure-from-{{Motion Revisited}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Schonberger, Johannes L. and Frahm, Jan-Michael},
  year = {2016},
  pages = {4104--4113},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.445},
  url = {http://ieeexplore.ieee.org/document/7780814/},
  urldate = {2024-01-18},
  abstract = {Incremental Structure-from-Motion is a prevalent strategy for 3D reconstruction from unordered image collections. While incremental reconstruction systems have tremendously advanced in all regards, robustness, accuracy, completeness, and scalability remain the key problems towards building a truly general-purpose pipeline. We propose a new SfM technique that improves upon the state of the art to make a further step towards this ultimate goal. The full reconstruction pipeline is released to the public as an open-source implementation.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/xuanenyun/Zotero/storage/3SLF7NDN/Schonberger 和 Frahm - 2016 - Structure-from-Motion Revisited.pdf}
}

@inproceedings{seitzComparisonEvaluationMultiView2006,
  title = {A {{Comparison}} and {{Evaluation}} of {{Multi-View Stereo Reconstruction Algorithms}}},
  booktitle = {2006 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} - {{Volume}} 1 ({{CVPR}}'06)},
  author = {Seitz, S.M. and Curless, B. and Diebel, J. and Scharstein, D. and Szeliski, R.},
  year = {2006},
  volume = {1},
  pages = {519--528},
  publisher = {{IEEE}},
  location = {{New York, NY, USA}},
  doi = {10.1109/CVPR.2006.19},
  url = {http://ieeexplore.ieee.org/document/1640800/},
  urldate = {2024-01-14},
  abstract = {This paper presents a quantitative comparison of several multi-view stereo reconstruction algorithms. Until now, the lack of suitable calibrated multi-view image datasets with known ground truth (3D shape models) has prevented such direct comparisons. In this paper, we first survey multi-view stereo algorithms and compare them qualitatively using a taxonomy that differentiates their key properties. We then describe our process for acquiring and calibrating multiview image datasets with high-accuracy ground truth and introduce our evaluation methodology. Finally, we present the results of our quantitative comparison of state-of-the-art multi-view stereo reconstruction algorithms on six benchmark datasets. The datasets, evaluation details, and instructions for submitting new models are available online at http://vision.middlebury.edu/mview.},
  eventtitle = {2006 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} - {{Volume}} 1 ({{CVPR}}'06)},
  isbn = {978-0-7695-2597-6},
  langid = {english},
  file = {/Users/xuanenyun/Zotero/storage/QNI6SP4W/Seitz 等 - 2006 - A Comparison and Evaluation of Multi-View Stereo R.pdf}
}

@inproceedings{tancikBlockNeRFScalableLarge2022,
  title = {Block-{{NeRF}}: {{Scalable Large Scene Neural View Synthesis}}},
  shorttitle = {Block-{{NeRF}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tancik, Matthew and Casser, Vincent and Yan, Xinchen and Pradhan, Sabeek and Mildenhall, Ben P. and Srinivasan, Pratul and Barron, Jonathan T. and Kretzschmar, Henrik},
  year = {2022},
  pages = {8238--8248},
  publisher = {{IEEE}},
  location = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.00807},
  url = {https://ieeexplore.ieee.org/document/9879943/},
  urldate = {2023-11-12},
  abstract = {We present Block-NeRF, a variant of Neural Radiance Fields that can represent large-scale environments. Specifically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained NeRFs. This decomposition decouples rendering time from scene size, enables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make NeRF robust to data captured over months under different environmental conditions. We add appearance embeddings, learned pose refinement, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to create the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco.},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  langid = {english},
  keywords = {ObsCite},
  file = {/Users/xuanenyun/Zotero/storage/NDKL3U7Q/Tancik 等 - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf}
}

@inproceedings{turkiMegaNeRFScalableConstruction2022,
  title = {Mega-{{NeRF}}: {{Scalable Construction}} of {{Large-Scale NeRFs}} for {{Virtual Fly- Throughs}}},
  shorttitle = {Mega-{{NeRF}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Turki, Haithem and Ramanan, Deva and Satyanarayanan, Mahadev},
  year = {2022},
  pages = {12912--12921},
  publisher = {{IEEE}},
  location = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.01258},
  url = {https://ieeexplore.ieee.org/document/9878491/},
  urldate = {2023-11-12},
  abstract = {We use neural radiance fields (NeRFs) to build interactive 3D environments from large-scale visual captures spanning buildings or even multiple city blocks collected primarily from drones. In contrast to single object scenes (on which NeRFs are traditionally evaluated), our scale poses multiple challenges including (1) the need to model thousands of images with varying lighting conditions, each of which capture only a small subset of the scene, (2) prohibitively large model capacities that make it infeasible to train on a single GPU, and (3) significant challenges for fast rendering that would enable interactive fly-throughs. To address these challenges, we begin by analyzing visibility statistics for large-scale scenes, motivating a sparse network structure where parameters are specialized to different regions of the scene. We introduce a simple geometric clustering algorithm for data parallelism that partitions training images (or rather pixels) into different NeRF submodules that can be trained in parallel. We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as well as against our own drone footage, improving training speed by 3x and PSNR by 12\%. We also evaluate recent NeRF fast renderers on top of Mega-NeRF and introduce a novel method that exploits temporal coherence. Our technique achieves a 40x speedup over conventional NeRF rendering while remaining within 0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  langid = {english},
  keywords = {ObsCite},
  file = {/Users/xuanenyun/Zotero/storage/EGAKLD3T/Turki 等 - 2022 - Mega-NeRF Scalable Construction of Large-Scale Ne.pdf}
}

@inproceedings{verbinRefNeRFStructuredViewDependent2022,
  title = {Ref-{{NeRF}}: {{Structured View-Dependent Appearance}} for {{Neural Radiance Fields}}},
  shorttitle = {Ref-{{NeRF}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Verbin, Dor and Hedman, Peter and Mildenhall, Ben and Zickler, Todd and Barron, Jonathan T. and Srinivasan, Pratul P.},
  year = {2022},
  pages = {5481--5490},
  publisher = {{IEEE}},
  location = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.00541},
  url = {https://ieeexplore.ieee.org/document/9879796/},
  urldate = {2024-01-27},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  file = {/Users/xuanenyun/Zotero/storage/FEDAMEL8/Verbin 等 - 2022 - Ref-NeRF Structured View-Dependent Appearance for.pdf}
}

@inproceedings{wangDeepVOEndtoEndVisual2017,
  title = {{{DeepVO}}: {{Towards End-to-End Visual Odometry}} with {{Deep Recurrent Convolutional Neural Networks}}},
  shorttitle = {{{DeepVO}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Wang, Sen and Clark, Ronald and Wen, Hongkai and Trigoni, Niki},
  year = {2017},
  eprint = {1709.08429},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {2043--2050},
  doi = {10.1109/ICRA.2017.7989236},
  url = {http://arxiv.org/abs/1709.08429},
  urldate = {2024-01-14},
  abstract = {This paper studies monocular visual odometry (VO) problem. Most of existing VO algorithms are developed under a standard pipeline including feature extraction, feature matching, motion estimation, local optimisation, etc. Although some of them have demonstrated superior performance, they usually need to be carefully designed and specifically fine-tuned to work well in different environments. Some prior knowledge is also required to recover an absolute scale for monocular VO. This paper presents a novel end-to-end framework for monocular VO by using deep Recurrent Convolutional Neural Networks (RCNNs) 1. Since it is trained and deployed in an end-to-end manner, it infers poses directly from a sequence of raw RGB images (videos) without adopting any module in the conventional VO pipeline. Based on the RCNNs, it not only automatically learns effective feature representation for the VO problem through Convolutional Neural Networks, but also implicitly models sequential dynamics and relations using deep Recurrent Neural Networks. Extensive experiments on the KITTI VO dataset show competitive performance to state-ofthe-art methods, verifying that the end-to-end Deep Learning technique can be a viable complement to the traditional VO systems.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/xuanenyun/Zotero/storage/4X4LT78X/Wang 等 - 2017 - DeepVO Towards End-to-End Visual Odometry with De.pdf}
}

@inproceedings{wangIBRNetLearningMultiView2021,
  title = {{{IBRNet}}: {{Learning Multi-View Image-Based Rendering}}},
  shorttitle = {{{IBRNet}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Qianqian and Wang, Zhicheng and Genova, Kyle and Srinivasan, Pratul and Zhou, Howard and Barron, Jonathan T. and Martin-Brualla, Ricardo and Snavely, Noah and Funkhouser, Thomas},
  year = {2021},
  pages = {4688--4697},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00466},
  url = {https://ieeexplore.ieee.org/document/9578424/},
  urldate = {2023-11-12},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  keywords = {ObsCite},
  file = {/Users/xuanenyun/Zotero/storage/43BWCEQ5/Wang 等 - 2021 - IBRNet Learning Multi-View Image-Based Rendering.pdf}
}

@inproceedings{wangNeRFFastNeural2023,
  title = {F {\textsuperscript{2}} -{{NeRF}}: {{Fast Neural Radiance Field Training}} with {{Free Camera Trajectories}}},
  shorttitle = {F {\textsuperscript{2}} -{{NeRF}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Peng and Liu, Yuan and Chen, Zhaoxi and Liu, Lingjie and Liu, Ziwei and Komura, Taku and Theobalt, Christian and Wang, Wenping},
  year = {2023},
  pages = {4150--4159},
  publisher = {{IEEE}},
  location = {{Vancouver, BC, Canada}},
  doi = {10.1109/CVPR52729.2023.00404},
  url = {https://ieeexplore.ieee.org/document/10204053/},
  urldate = {2023-11-12},
  eventtitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {9798350301298},
  langid = {english},
  keywords = {ObsCite},
  file = {/Users/xuanenyun/Zotero/storage/NWT9BDI2/Wang 等 - 2023 - F 2 -NeRF Fast Neural Radiance Field T.pdf}
}

@online{wangPixel2MeshGenerating3D2018,
  title = {{{Pixel2Mesh}}: {{Generating 3D Mesh Models}} from {{Single RGB Images}}},
  shorttitle = {{{Pixel2Mesh}}},
  author = {Wang, Nanyang and Zhang, Yinda and Li, Zhuwen and Fu, Yanwei and Liu, Wei and Jiang, Yu-Gang},
  year = {2018},
  eprint = {1804.01654},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1804.01654},
  urldate = {2024-01-14},
  abstract = {We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image. Limited by the nature of deep neural network, previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D mesh in a graph-based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid, leveraging perceptual features extracted from the input image. We adopt a coarse-to-fine strategy to make the whole deformation procedure stable, and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry. Extensive experiments show that our method not only qualitatively produces mesh model with better details, but also achieves higher 3D shape estimation accuracy compared to the state-of-the-art.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/xuanenyun/Zotero/storage/A79EHCG2/Wang 等 - 2018 - Pixel2Mesh Generating 3D Mesh Models from Single .pdf}
}

@article{westobyStructurefromMotionPhotogrammetryLowcost2012,
  title = {‘{{Structure-from-Motion}}’ Photogrammetry: {{A}} Low-Cost, Effective Tool for Geoscience Applications},
  shorttitle = {‘{{Structure-from-Motion}}’ Photogrammetry},
  author = {Westoby, M.J. and Brasington, J. and Glasser, N.F. and Hambrey, M.J. and Reynolds, J.M.},
  year = {2012},
  journaltitle = {Geomorphology},
  shortjournal = {Geomorphology},
  volume = {179},
  pages = {300--314},
  issn = {0169555X},
  doi = {10.1016/j.geomorph.2012.08.021},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0169555X12004217},
  urldate = {2024-01-18},
  abstract = {High-resolution topographic surveying is traditionally associated with high capital and logistical costs, so that data acquisition is often passed on to specialist third party organisations. The high costs of data collection are, for many applications in the earth sciences, exacerbated by the remoteness and inaccessibility of many field sites, rendering cheaper, more portable surveying platforms (i.e. terrestrial laser scanning or GPS) impractical. This paper outlines a revolutionary, low-cost, user-friendly photogrammetric technique for obtaining high-resolution datasets at a range of scales, termed ‘Structure-from-Motion’ (SfM). Traditional softcopy photogrammetric methods require the 3-D location and pose of the camera(s), or the 3-D location of ground control points to be known to facilitate scene triangulation and reconstruction. In contrast, the SfM method solves the camera pose and scene geometry simultaneously and automatically, using a highly redundant bundle adjustment based on matching features in multiple overlapping, offset images. A comprehensive introduction to the technique is presented, followed by an outline of the methods used to create high-resolution digital elevation models (DEMs) from extensive photosets obtained using a consumer-grade digital camera. As an initial appraisal of the technique, an SfM-derived DEM is compared directly with a similar model obtained using terrestrial laser scanning. This intercomparison reveals that decimetre-scale vertical accuracy can be achieved using SfM even for sites with complex topography and a range of land-covers. Example applications of SfM are presented for three contrasting landforms across a range of scales including; an exposed rocky coastal cliff; a breached moraine-dam complex; and a glacially-sculpted bedrock ridge. The SfM technique represents a major advancement in the field of photogrammetry for geoscience applications. Our results and experiences indicate SfM is an inexpensive, effective, and flexible approach to capturing complex topography.},
  langid = {english},
  file = {/Users/xuanenyun/Zotero/storage/33V3V2Z2/Westoby 等 - 2012 - ‘Structure-from-Motion’ photogrammetry A low-cost.pdf}
}

@online{WoZhengLiLiaoJin300PianWenXianXieLiaoYiPianSCIWenXianZongShuZhiHu,
  title = {我整理了近300篇文献，{{写了一篇SCI文献综述}} - 知乎},
  url = {https://zhuanlan.zhihu.com/p/336779180},
  urldate = {2024-01-22},
  file = {/Users/xuanenyun/Zotero/storage/CXBGPPHX/336779180.html}
}

@inproceedings{xiangPASCALBenchmark3D2014,
  title = {Beyond {{PASCAL}}: {{A}} Benchmark for {{3D}} Object Detection in the Wild},
  shorttitle = {Beyond {{PASCAL}}},
  booktitle = {{{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Xiang, Yu and Mottaghi, Roozbeh and Savarese, Silvio},
  year = {2014},
  pages = {75--82},
  publisher = {{IEEE}},
  location = {{Steamboat Springs, CO, USA}},
  doi = {10.1109/WACV.2014.6836101},
  url = {https://ieeexplore.ieee.org/document/6836101},
  urldate = {2024-01-25},
  eventtitle = {2014 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-4799-4985-4}
}

@inproceedings{xuPointNeRFPointbasedNeural2022,
  title = {Point-{{NeRF}}: {{Point-based Neural Radiance Fields}}},
  shorttitle = {Point-{{NeRF}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xu, Qiangeng and Xu, Zexiang and Philip, Julien and Bi, Sai and Shu, Zhixin and Sunkavalli, Kalyan and Neumann, Ulrich},
  year = {2022},
  pages = {5428--5438},
  publisher = {{IEEE}},
  location = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.00536},
  url = {https://ieeexplore.ieee.org/document/9880452/},
  urldate = {2024-01-28},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  file = {/Users/xuanenyun/Zotero/storage/2Z75F3ZV/Xu 等 - 2022 - Point-NeRF Point-based Neural Radiance Fields.pdf}
}

@incollection{yaoMVSNetDepthInference2018,
  title = {{{MVSNet}}: {{Depth Inference}} for {{Unstructured Multi-view Stereo}}},
  shorttitle = {{{MVSNet}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Yao, Yao and Luo, Zixin and Li, Shiwei and Fang, Tian and Quan, Long},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11212},
  pages = {785--801},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01237-3_47},
  url = {https://link.springer.com/10.1007/978-3-030-01237-3_47},
  urldate = {2024-01-28},
  isbn = {978-3-030-01236-6 978-3-030-01237-3},
  langid = {english},
  file = {/Users/xuanenyun/Zotero/storage/82YEFKH9/Yao 等 - 2018 - MVSNet Depth Inference for Unstructured Multi-vie.pdf}
}

@online{yuPlenoxelsRadianceFields2021,
  title = {Plenoxels: {{Radiance Fields}} without {{Neural Networks}}},
  shorttitle = {Plenoxels},
  author = {Yu, Alex and Fridovich-Keil, Sara and Tancik, Matthew and Chen, Qinhong and Recht, Benjamin and Kanazawa, Angjoo},
  year = {2021},
  eprint = {2112.05131},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.05131},
  urldate = {2024-01-28},
  abstract = {We introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/xuanenyun/Zotero/storage/KSC33TMU/Yu 等 - 2021 - Plenoxels Radiance Fields without Neural Networks.pdf}
}

@online{zhangNeRFAnalyzingImproving2020,
  title = {{{NeRF}}++: {{Analyzing}} and {{Improving Neural Radiance Fields}}},
  shorttitle = {{{NeRF}}++},
  author = {Zhang, Kai and Riegler, Gernot and Snavely, Noah and Koltun, Vladlen},
  year = {2020},
  eprint = {2010.07492},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.07492},
  urldate = {2023-11-12},
  abstract = {Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360◦ capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multilayer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF’s success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360◦ captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,ObsCite},
  file = {/Users/xuanenyun/Zotero/storage/E7VCQ373/Zhang 等 - 2020 - NeRF++ Analyzing and Improving Neural Radiance Fi.pdf}
}

@online{parkCamPCameraPreconditioning2023,
  title = {{{CamP}}: {{Camera Preconditioning}} for {{Neural Radiance Fields}}},
  shorttitle = {{{CamP}}},
  author = {Park, Keunhong and Henzler, Philipp and Mildenhall, Ben and Barron, Jonathan T. and Martin-Brualla, Ricardo},
  year = {2023},
  eprint = {2308.10902},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2308.10902},
  urldate = {2024-01-28},
  abstract = {Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D scene reconstructions of objects and large-scale scenes. However, NeRFs require accurate camera parameters as input -- inaccurate camera parameters result in blurry renderings. Extrinsic and intrinsic camera parameters are usually estimated using Structure-from-Motion (SfM) methods as a pre-processing step to NeRF, but these techniques rarely yield perfect estimates. Thus, prior works have proposed jointly optimizing camera parameters alongside a NeRF, but these methods are prone to local minima in challenging settings. In this work, we analyze how different camera parameterizations affect this joint optimization problem, and observe that standard parameterizations exhibit large differences in magnitude with respect to small perturbations, which can lead to an ill-conditioned optimization problem. We propose using a proxy problem to compute a whitening transform that eliminates the correlation between camera parameters and normalizes their effects, and we propose to use this transform as a preconditioner for the camera parameters during joint optimization. Our preconditioned camera optimization significantly improves reconstruction quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE) by 67\% compared to state-of-the-art NeRF approaches that do not optimize for cameras like Zip-NeRF, and by 29\% relative to state-of-the-art joint optimization approaches using the camera parameterization of SCNeRF. Our approach is easy to implement, does not significantly increase runtime, can be applied to a wide variety of camera parameterizations, and can straightforwardly be incorporated into other NeRF-like models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/xuanenyun/Zotero/storage/6WSHLAUS/Park 等 - 2023 - CamP Camera Preconditioning for Neural Radiance F.pdf}
}
