@online{DiffusionModelXiangJie,
  title = {Diffusion Model è¯¦è§£ï¼šç›´è§‚ç†è§£ã€æ•°å­¦åŸç†ã€PyTorch å®ç°},
  url = {https://zhuanlan.zhihu.com/p/638442430},
  urldate = {2024-02-25},
  abstract = {æœ¬æ–‡å…¥é€‰ã€æŠ€æœ¯å†™ä½œè®­ç»ƒè¥ã€‘ä¼˜ç§€ç»“è¥ä½œå“ï¼Œä½œè€…ï¼š @å‘¨å¼ˆå¸† åœ¨è¿‡å»çš„å¤§åŠå¹´é‡Œï¼Œä»¥Stable Diffusionä¸ºä»£è¡¨çš„AIç»˜ç”»æ˜¯ä¸–ç•Œä¸Šæœ€ä¸ºç«çƒ­çš„AIæ–¹å‘ä¹‹ä¸€ã€‚æˆ–è®¸å¤§å®¶ä¼šæœ‰ç–‘é—®ï¼ŒStable Diffusioné‡Œçš„è¿™ä¸ª\&\#34;Diffusion\&\#34;æ˜¯â€¦},
  langid = {chinese},
  organization = {{çŸ¥ä¹ä¸“æ }},
  file = {/Users/xuanenyun/Zotero/storage/CEU6RLSG/638442430.html}
}

@online{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  eprint = {2006.11239},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2024-02-25},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/xuanenyun/Zotero/storage/2TDMZNVC/Ho ç­‰ - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@online{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  eprint = {1503.03585},
  eprinttype = {arxiv},
  eprintclass = {cond-mat, q-bio, stat},
  url = {http://arxiv.org/abs/1503.03585},
  urldate = {2024-02-26},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/xuanenyun/Zotero/storage/6DHNE49Q/Sohl-Dickstein ç­‰ - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf}
}

@online{chenSurvey3DGaussian2024,
  title = {A {{Survey}} on {{3D Gaussian Splatting}}},
  author = {Chen, Guikun and Wang, Wenguan},
  year = {2024},
  eprint = {2401.03890},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2401.03890},
  urldate = {2024-01-16},
  abstract = {3D Gaussian splatting (3D GS) has recently emerged as a transformative technique in the explicit radiance field and computer graphics landscape. This innovative approach, characterized by the utilization of millions of 3D Gaussians, represents a significant departure from the neural radiance field (NeRF) methodologies, which predominantly use implicit, coordinate-based models to map spatial coordinates to pixel values. 3D GS, with its explicit scene representations and differentiable rendering algorithms, not only promises real-time rendering capabilities but also introduces unprecedented levels of control and editability. This positions 3D GS as a potential game-changer for the next generation of 3D reconstruction and representation. In the present paper, we provide the first systematic overview of the recent developments and critical contributions in the domain of 3D GS. We begin with a detailed exploration of the underlying principles and the driving forces behind the advent of 3D GS, setting the stage for understanding its significance. A focal point of our discussion is the practical applicability of 3D GS. By facilitating real-time performance, 3D GS opens up a plethora of applications, ranging from virtual reality to interactive media and beyond. This is complemented by a comparative analysis of leading 3D GS models, evaluated across various benchmark tasks to highlight their performance and practical utility. The survey concludes by identifying current challenges and suggesting potential avenues for future research in this domain. Through this survey, we aim to provide a valuable resource for both newcomers and seasoned researchers, fostering further exploration and advancement in applicable and explicit radiance field representation.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Multimedia,survey},
  note = {Comment: Ongoing project},
  file = {/Users/xuanenyun/Zotero/storage/E3YBKX67/Chen å’Œ Wang - 2024 - A Survey on 3D Gaussian Splatting.pdf;/Users/xuanenyun/Zotero/storage/QHJKSYRJ/2401.html}
}

@online{chenTextto3DUsingGaussian2023,
  title = {Text-to-{{3D}} Using {{Gaussian Splatting}}},
  author = {Chen, Zilong and Wang, Feng and Liu, Huaping},
  year = {2023},
  eprint = {2309.16585},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.16585},
  urldate = {2024-02-22},
  abstract = {In this paper, we present Gaussian Splatting based text-to-3D generation (GSGEN), a novel approach for generating high-quality 3D objects. Previous methods suffer from inaccurate geometry and limited fidelity due to the absence of 3D prior and proper representation. We leverage 3D Gaussian Splatting, a recent state-of-the-art representation, to address existing shortcomings by exploiting the explicit nature that enables the incorporation of 3D prior. Specifically, our method adopts a progressive optimization strategy, which includes a geometry optimization stage and an appearance refinement stage. In geometry optimization, a coarse representation is established under a 3D geometry prior along with the ordinary 2D SDS loss, ensuring a sensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an iterative refinement to enrich details. In this stage, we increase the number of Gaussians by compactness-based densification to enhance continuity and improve fidelity. With these designs, our approach can generate 3D content with delicate details and more accurate geometry. Extensive evaluations demonstrate the effectiveness of our method, especially for capturing high-frequency components. Our code is available at https://github.com/gsgen3d/gsgen/.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project page: https://gsgen3d.github.io. Code: https://github.com/gsgen3d/gsgen},
  file = {/Users/xuanenyun/Zotero/storage/43S723BM/Chen ç­‰ - 2023 - Text-to-3D using Gaussian Splatting.pdf}
}

@online{kerbl3DGaussianSplatting2023,
  title = {{{3D Gaussian Splatting}} for {{Real-Time Radiance Field Rendering}}},
  author = {Kerbl, Bernhard and Kopanas, Georgios and LeimkÃ¼hler, Thomas and Drettakis, George},
  year = {2023},
  eprint = {2308.04079},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2308.04079},
  urldate = {2024-01-15},
  abstract = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time ({$>$}= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  note = {\section{æ³¨é‡Š\\
(2024/1/18 ä¸‹åˆ1:28:48)}

\par
â€œSimilarly, the most efficient radiance field solutions to date build on continuous representations by interpolating values stored in, e.g., voxel [Fridovich-Keil and Yu et al. 2022] or hash [MÃ¼ller et al. 2022] grids or points [Xu et al. 2022].\\
ğŸ”¤ç±»ä¼¼åœ°ï¼Œè¿„ä»Šä¸ºæ­¢æœ€æœ‰æ•ˆçš„è¾å°„åœºè§£å†³æ–¹æ¡ˆé€šè¿‡å¯¹å­˜å‚¨åœ¨ä½“ç´ ç­‰ä¸­çš„å€¼è¿›è¡Œæ’å€¼æ¥æ„å»ºè¿ç»­è¡¨ç¤º[Fridovich-Keil å’Œ Yu ç­‰äººã€‚ 2022] æˆ–å“ˆå¸Œ [MÃ¼ller ç­‰äººã€‚ 2022]ç½‘æ ¼æˆ–ç‚¹[Xuç­‰äººã€‚ 2022]ã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 11)
\par
â€œWe first introduce 3D Gaussians as a flexible and expressive scene representation.\\
ğŸ”¤æˆ‘ä»¬é¦–å…ˆå¼•å…¥3D Gaussianä½œä¸ºä¸€ç§çµæ´»ä¸”å…·æœ‰è¡¨è¾¾èƒ½åŠ›çš„åœºæ™¯è¡¨ç¤ºã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 12)
\par
â€œWe start with the same input as previous NeRF-like methods, i.e., cameras calibrated with Structure-from-Motion (SfM) [Snavely et al. 2006] and initialize the set of 3D Gaussians with the sparse point cloud produced for free as part of the SfM process.\\
ğŸ”¤æˆ‘ä»¬ä»ä¸å…ˆå‰çš„NeRF - likeæ–¹æ³•ç›¸åŒçš„è¾“å…¥å¼€å§‹ï¼Œå³ä½¿ç”¨è¿åŠ¨æ¢å¤ç»“æ„( Structure-from- Motionï¼ŒSfM ) [æ–¯å†…å¤«åˆ©ç­‰2006]å¯¹ç›¸æœºè¿›è¡Œæ ‡å®šï¼Œå¹¶ä½¿ç”¨å…è´¹ç”Ÿæˆçš„ç¨€ç–ç‚¹äº‘åˆå§‹åŒ–ä¸‰ç»´é«˜æ–¯ç‚¹é›†ä½œä¸ºSfMè¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 12)
\par
â€œIn contrast to most point-based solutions that require Multi-View Stereo (MVS) data [Aliev et al. 2020; Kopanas et al. 2021; RÃ¼ckert et al. 2022], we achieve high-quality results with only SfM points as input.\\
ğŸ”¤ä¸å¤§å¤šæ•°åŸºäºç‚¹çš„è§£å†³æ–¹æ¡ˆéœ€è¦å¤šè§†ç«‹ä½“( Multi-View Stereoï¼ŒMVS )æ•°æ®[ Aliev et al 2020 ; Kopanas et al 2021 ;å•å…‹ç‰¹et al 2022]ä¸åŒï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨SfMç‚¹ä½œä¸ºè¾“å…¥å°±å¯ä»¥è·å¾—é«˜è´¨é‡çš„ç»“æœã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 12)
\par
â€œWe show that 3D Gaussians are an excellent choice, since they are a differentiable volumetric representation, but they can also be rasterized very efficiently by projecting them to 2D, and applying standard ğ›¼-blending, using an equivalent image formation model as NeRF.\\
ğŸ”¤æˆ‘ä»¬è¡¨æ˜3Dé«˜æ–¯æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ä¸€ä¸ªå¯å¾®çš„ä½“ç§¯è¡¨ç¤ºï¼Œä½†æ˜¯é€šè¿‡å°†å®ƒä»¬æŠ•å½±åˆ°2Dï¼Œå¹¶åº”ç”¨æ ‡å‡†çš„Î±æ··åˆï¼Œä½¿ç”¨ç­‰æ•ˆçš„å›¾åƒå½¢æˆæ¨¡å‹NeRFï¼Œå®ƒä»¬ä¹Ÿå¯ä»¥éå¸¸æœ‰æ•ˆåœ°æ …æ ¼åŒ–ã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 12)
\par
â€œThe second component of our method is optimization of the properties of the 3D Gaussians â€“ 3D position, opacity ğ›¼, anisotropic covariance, and spherical harmonic (SH) coefficients â€“ interleaved with adaptive density control steps, where we add and occasionally remove 3D Gaussians during optimization.\\
ğŸ”¤æˆ‘ä»¬æ–¹æ³•çš„ç¬¬äºŒä¸ªç»„æˆéƒ¨åˆ†æ˜¯ä¼˜åŒ–ä¸‰ç»´é«˜æ–¯çš„æ€§è´¨- -ä¸‰ç»´ä½ç½®ã€ä¸é€æ˜åº¦Î±ã€å„å‘å¼‚æ€§åæ–¹å·®å’Œçƒè°( SH )ç³»æ•°- -ä¸è‡ªé€‚åº”å¯†åº¦æ§åˆ¶æ­¥éª¤äº¤ç»‡åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æ·»åŠ å’Œå¶å°”åˆ é™¤ä¸‰ç»´é«˜æ–¯ã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 12)
\par
â€œThe third and final element of our method is our real-time rendering solution that uses fast GPU sorting algorithms and is inspired by tile-based rasterization, following recent work [Lassner and Zollhofer 2021].\\
ğŸ”¤æˆ‘ä»¬æ–¹æ³•çš„ç¬¬ä¸‰ä¸ªä¹Ÿæ˜¯æœ€åä¸€ä¸ªå…ƒç´ æ˜¯æˆ‘ä»¬çš„å®æ—¶æ¸²æŸ“è§£å†³æ–¹æ¡ˆï¼Œå®ƒä½¿ç”¨äº†å¿«é€Ÿçš„GPUæ’åºç®—æ³•ï¼Œå¹¶å—åˆ°åŸºäºç“¦ç‰‡çš„æ …æ ¼åŒ–çš„å¯å‘ï¼Œè¿™æ˜¯åœ¨æœ€è¿‘çš„å·¥ä½œ[æ‹‰æ–¯çº³å’ŒZollhofer 2021]ä¹‹åã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 12)
\par
â€œThe introduction of anisotropic 3D Gaussians as a high-quality, unstructured representation of radiance fields. â€¢ An optimization method of 3D Gaussian properties, interleaved with adaptive density control that creates high-quality representations for captured scenes. â€¢ A fast, differentiable rendering approach for the GPU, which is visibility-aware, allows anisotropic splatting and fast backpropagation to achieve high-quality novel view synthesis.\\
ğŸ”¤å¼•å…¥å„å‘å¼‚æ€§3Dé«˜æ–¯ä½œä¸ºè¾å°„åœºçš„é«˜è´¨é‡ã€éç»“æ„åŒ–è¡¨ç¤ºã€‚Â·ä¸€ç§3Dé«˜æ–¯ç‰¹æ€§çš„ä¼˜åŒ–æ–¹æ³•ï¼Œä¸è‡ªé€‚åº”å¯†åº¦æ§åˆ¶äº¤ç»‡åœ¨ä¸€èµ·ï¼Œä¸ºæ•è·çš„åœºæ™¯åˆ›å»ºé«˜è´¨é‡çš„è¡¨ç¤ºã€‚Â·ä¸€ç§å¯è§†æ€§æ„ŸçŸ¥çš„GPUå¿«é€Ÿã€å¯å¾®çš„æ¸²æŸ“æ–¹æ³•ï¼Œå…è®¸å„å‘å¼‚æ€§æ•£å°„å’Œå¿«é€Ÿåå‘ä¼ æ’­ï¼Œä»¥å®ç°é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 12)
\par
â€œFor complete coverage of the field, please see the excellent recent surveys [Tewari et al. 2022; Xie et al. 2022].\\
ğŸ”¤ä¸ºå…¨é¢è¦†ç›–è¯¥é¢†åŸŸï¼Œè¯·å‚é˜…è¿‘æœŸä¼˜ç§€è°ƒæŸ¥[ Tewari et al 2022 ; Xie et al 2022]ã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 12)
\par
â€œThe advent of Structure-from-Motion (SfM) [Snavely et al. 2006] enabled an entire new domain where a collection of photos could be used to synthesize novel views\\
ğŸ”¤è¿åŠ¨æ¢å¤ç»“æ„( Structure-from-motionï¼ŒSfM ) [æ–¯å†…å¤«åˆ©ç­‰2006]çš„å‡ºç°ä½¿ä¸€ä¸ªå…¨æ–°çš„é¢†åŸŸæˆä¸ºå¯èƒ½ï¼Œåœ¨è¿™ä¸ªé¢†åŸŸä¸­ï¼Œç…§ç‰‡çš„é›†åˆå¯ä»¥ç”¨æ¥åˆæˆæ–°çš„è§†å›¾ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 12)
\par
â€œthe current state-of-the-art in image quality for novel-view synthesis is Mip-NeRF360 [Barron et al. 2022].\\
ğŸ”¤ç›®å‰ï¼Œç”¨äºæ–°è§†ç‚¹åˆæˆçš„å›¾åƒè´¨é‡æœ€å¥½çš„æ˜¯Mip - NeRF360 [ Barron et al 2022]ã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 12)
\par
â€œIn addition, image quality is limited in large part by the choice of the structured grids used for acceleration, and rendering speed is hindered by the need to query many samples for a given ray-marching step.\\
ğŸ”¤æ­¤å¤–ï¼Œå›¾åƒè´¨é‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—é™äºç”¨äºåŠ é€Ÿçš„ç»“æ„åŒ–ç½‘æ ¼çš„é€‰æ‹©ï¼Œç»˜åˆ¶é€Ÿåº¦ä¹Ÿå› éœ€è¦æŸ¥è¯¢ç»™å®šå°„çº¿ç§»åŠ¨æ­¥çš„å¤šä¸ªæ ·æœ¬è€Œå—åˆ°é˜»ç¢ã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 13)
\par
â€œWhile true to the underlying data, point sample rendering suffers from holes, causes aliasing, and is strictly discontinuous.\\
ğŸ”¤è€Œå¯¹äºåº•å±‚æ•°æ®æ¥è¯´ï¼Œç‚¹æ ·æœ¬æ¸²æŸ“ä¼šäº§ç”Ÿç©ºæ´ï¼Œå¯¼è‡´èµ°æ ·ï¼Œå¹¶ä¸”æ˜¯ä¸¥æ ¼ä¸è¿ç»­çš„ã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 13)
\par
â€œSeminal work on high-quality point-based rendering addresses these issues by â€œsplattingâ€ point primitives with an extent larger than a pixel, e.g., circular or elliptic discs, ellipsoids, or surfels [Botsch et al. 2005; Pfister et al. 2000; Ren et al. 2002; Zwicker et al. 2001b].\\
ğŸ”¤é«˜è´¨é‡çš„åŸºäºç‚¹ç»˜åˆ¶çš„åç»­å·¥ä½œé€šè¿‡"é£æº…"ç¨‹åº¦å¤§äºä¸€ä¸ªåƒç´ çš„ç‚¹åŸºå…ƒæ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä¾‹å¦‚åœ†å½¢æˆ–æ¤­åœ†å½¢çš„åœ†ç›˜ã€æ¤­çƒæˆ–é¢å…ƒ[ Botsch et al 2005 ;è²æ–¯ç‰¹et al 2000 ; Ren et al 2002 ;èŒ¨ç»´å…‹å°”et al 2001b]ã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 13)
\par
(Kerbl ç­‰, 2023, p. 13)
\par
(Kerbl ç­‰, 2023, p. 13)
\par
â€œNeRFs are a continuous representation implicitly representing empty/occupied space; expensive random sampling is required to find the samples in Eq. 2 with consequent noise and computational expense.\\
ğŸ”¤NeRFsæ˜¯ä¸€ç§éšå«åœ°è¡¨å¾ç©º/è¢«å ç”¨ç©ºé—´çš„è¿ç»­è¡¨å¾ï¼›ä¸ºäº†æ‰¾åˆ°æ–¹ç¨‹ä¸­çš„æ ·æœ¬ï¼Œéœ€è¦è¿›è¡Œæ˜‚è´µçš„éšæœºæŠ½æ ·ã€‚2ï¼Œä¼´éšç€å™ªå£°å’Œè®¡ç®—æˆæœ¬ã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 13)
\par
â€œOur rasterization respects visibility order in contrast to their order-independent method.\\
ğŸ”¤ä¸å®ƒä»¬çš„é¡ºåºæ— å…³æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ …æ ¼åŒ–å°Šé‡å¯è§æ€§é¡ºåºã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 13)
\par
â€œWe use 3D Gaussians for a more flexible scene representation, avoiding the need for MVS geometry and achieving real-time rendering thanks to our tile-based rendering algorithm for the projected Gaussians.\\
ğŸ”¤æˆ‘ä»¬ä½¿ç”¨3Dé«˜æ–¯è¿›è¡Œæ›´çµæ´»çš„åœºæ™¯è¡¨ç¤ºï¼Œé¿å…äº†å¯¹MVSå‡ ä½•çš„éœ€è¦ï¼Œå¹¶ä¸”ç”±äºæˆ‘ä»¬åŸºäºç“¦ç‰‡çš„æŠ•å½±é«˜æ–¯æ¸²æŸ“ç®—æ³•ï¼Œå®ç°äº†å®æ—¶æ¸²æŸ“ã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 13)
\par
â€œFrom these points we create a set of 3D Gaussians (Sec. 4), defined by a position (mean), covariance matrix and opacity ğ›¼, that allows a very flexible optimization regime\\
ğŸ”¤ä»è¿™äº›ç‚¹å‡ºå‘ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ç»„ä¸‰ç»´é«˜æ–¯(ç¬¬4èŠ‚)ï¼Œç”±ä½ç½®(å‡å€¼)ã€åæ–¹å·®çŸ©é˜µå’Œä¸é€æ˜åº¦Î±å®šä¹‰ï¼Œå…è®¸éå¸¸çµæ´»çš„ä¼˜åŒ–æœºåˆ¶ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 14)
\par
â€œThe directional appearance component (color) of the radiance field is represented via spherical harmonics (SH)\\
ğŸ”¤è¾å°„åœºçš„æ–¹å‘å¤–è§‚åˆ†é‡(é¢œè‰²)ç”¨çƒè°å‡½æ•°( SH )è¡¨ç¤ºã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 14)
\par
â€œOur Gaussians are defined by a full 3D covariance matrix Î£ defined in world space [Zwicker et al. 2001a] centered at point (mean) ğœ‡\\
ğŸ”¤æˆ‘ä»¬çš„é«˜æ–¯ç”±å®šä¹‰åœ¨ä¸–ç•Œç©ºé—´[èŒ¨ç»´å…‹å°”ç­‰2001a]ä¸­ä»¥ç‚¹(å‡å€¼) Î¼ä¸ºä¸­å¿ƒçš„å…¨ä¸‰ç»´åæ–¹å·®çŸ©é˜µÎ£å®šä¹‰ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 14)
\par
(Kerbl ç­‰, 2023, p. 14)
\par
â€œGiven a viewing transformation ğ‘Š the covariance matrix Î£â€² in camera coordinates is given as follows:\\
ğŸ”¤ç»™å®šä¸€ä¸ªè§‚æµ‹å˜æ¢Wï¼Œç›¸æœºåæ ‡ä¸‹çš„åæ–¹å·®çŸ©é˜µÎ£â€²ä¸ºğŸ”¤â€ (Kerbl ç­‰, 2023, p. 14)
\par
(Kerbl ç­‰, 2023, p. 14)
\par
â€œThe core of our approach is the optimization step, which creates a dense set of 3D Gaussians accurately representing the scene for free-view synthesis.\\
ğŸ”¤æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¼˜åŒ–æ­¥éª¤ï¼Œè¯¥æ­¥éª¤åˆ›å»ºäº†ä¸€ä¸ªç¨ å¯†çš„3Dé«˜æ–¯é›†åˆï¼Œå‡†ç¡®åœ°è¡¨ç¤ºäº†è‡ªç”±è§†å›¾åˆæˆçš„åœºæ™¯ã€‚ğŸ”¤â€ (Kerbl ç­‰, 2023, p. 14)
\par
(Kerbl ç­‰, 2023, p. 15)
\par
(Kerbl ç­‰, 2023, p. 23)
\par
(Kerbl ç­‰, 2023, p. 24)
\par
Comment: https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/},
  file = {/Users/xuanenyun/Zotero/storage/ALZIBKB6/Kerbl ç­‰ - 2023 - 3D Gaussian Splatting for Real-Time Radiance Field.pdf}
}

@inproceedings{linMagic3DHighResolutionTextto3D2023,
  title = {{{Magic3D}}: {{High-Resolution Text-to-3D Content Creation}}},
  shorttitle = {{{Magic3D}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and Takikawa, Towaki and Zeng, Xiaohui and Huang, Xun and Kreis, Karsten and Fidler, Sanja and Liu, Ming-Yu and Lin, Tsung-Yi},
  year = {2023},
  pages = {300--309},
  publisher = {{IEEE}},
  location = {{Vancouver, BC, Canada}},
  doi = {10.1109/CVPR52729.2023.00037},
  url = {https://ieeexplore.ieee.org/document/10203601/},
  urldate = {2024-02-25},
  eventtitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {9798350301298},
  file = {/Users/xuanenyun/Zotero/storage/VRTTJBAR/Lin ç­‰ - 2023 - Magic3D High-Resolution Text-to-3D Content Creati.pdf}
}

@online{nicholPointESystemGenerating2022,
  title = {Point-{{E}}: {{A System}} for {{Generating 3D Point Clouds}} from {{Complex Prompts}}},
  shorttitle = {Point-{{E}}},
  author = {Nichol, Alex and Jun, Heewoo and Dhariwal, Prafulla and Mishkin, Pamela and Chen, Mark},
  year = {2022},
  eprint = {2212.08751},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2212.08751},
  urldate = {2024-02-25},
  abstract = {While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https: //github.com/openai/point-e. virtual reality, gaming, and industrial design.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: 8 pages, 11 figures},
  file = {/Users/xuanenyun/Zotero/storage/DDHXP49Q/Nichol ç­‰ - 2022 - Point-E A System for Generating 3D Point Clouds f.pdf}
}

@online{niedermayrCompressed3DGaussian2024,
  title = {Compressed {{3D Gaussian Splatting}} for {{Accelerated Novel View Synthesis}}},
  author = {Niedermayr, Simon and Stumpfegger, Josef and Westermann, RÃ¼diger},
  year = {2024},
  eprint = {2401.02436},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2401.02436},
  urldate = {2024-02-22},
  abstract = {Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian splat representation has been introduced for novel view synthesis from sparse image sets. Making such representations suitable for applications like network streaming and rendering on low-power devices requires significantly reduced memory consumption as well as improved rendering efficiency. We propose a compressed 3D Gaussian splat representation that utilizes sensitivityaware vector clustering with quantization-aware training to compress directional colors and Gaussian parameters. The learned codebooks have low bitrates and achieve a compression rate of up to 31Ã— on real-world scenes with only minimal degradation of visual quality. We demonstrate that the compressed splat representation can be efficiently rendered with hardware rasterization on lightweight GPUs at up to 4Ã— higher framerates than reported via an optimized GPU compute pipeline. Extensive experiments across multiple datasets demonstrate the robustness and rendering speed of the proposed approach.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/xuanenyun/Zotero/storage/S6CBPSNJ/Niedermayr ç­‰ - 2024 - Compressed 3D Gaussian Splatting for Accelerated N.pdf}
}

@online{pooleDreamFusionTextto3DUsing2022,
  title = {{{DreamFusion}}: {{Text-to-3D}} Using {{2D Diffusion}}},
  shorttitle = {{{DreamFusion}}},
  author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
  year = {2022},
  eprint = {2209.14988},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2209.14988},
  urldate = {2024-02-25},
  abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors. See dreamfusion3d.github.io for a more immersive view into our 3D results.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: see project page at https://dreamfusion3d.github.io/},
  file = {/Users/xuanenyun/Zotero/storage/RY2CBKTI/Poole ç­‰ - 2022 - DreamFusion Text-to-3D using 2D Diffusion.pdf}
}

@online{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, BjÃ¶rn},
  year = {2022},
  eprint = {2112.10752},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.10752},
  urldate = {2024-02-25},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: CVPR 2022},
  file = {/Users/xuanenyun/Zotero/storage/UUCC98U3/Rombach ç­‰ - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf}
}

@online{sahariaPhotorealisticTexttoImageDiffusion2022,
  title = {Photorealistic {{Text-to-Image Diffusion Models}} with {{Deep Language Understanding}}},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  eprint = {2205.11487},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.11487},
  urldate = {2024-02-26},
  abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@online{WoZhengLiLiaoJin300PianWenXianXieLiaoYiPianSCIWenXianZongShuZhiHu,
  title = {æˆ‘æ•´ç†äº†è¿‘300ç¯‡æ–‡çŒ®ï¼Œ{{å†™äº†ä¸€ç¯‡SCIæ–‡çŒ®ç»¼è¿°}} - çŸ¥ä¹},
  url = {https://zhuanlan.zhihu.com/p/336779180},
  urldate = {2024-01-22},
  file = {/Users/xuanenyun/Zotero/storage/CXBGPPHX/336779180.html}
}

@article{yangGaussianObjectJustTaking,
  title = {{{GaussianObject}}: {{Just Taking Four Images}} to {{Get A High-Quality 3D Object}} with {{Gaussian Splatting}}},
  author = {Yang, Chen and Li, Sikuang and Fang, Jiemin and Liang, Ruofan and Xie, Lingxi and Zhang, Xiaopeng and Shen, Wei and Tian, Qi},
  langid = {english},
  file = {/Users/xuanenyun/Zotero/storage/ZG5SCBKW/Yang ç­‰ - GaussianObject Just Taking Four Images to Get A H.pdf}
}

@online{zhouGALA3DTextto3DComplex2024,
  title = {{{GALA3D}}: {{Towards Text-to-3D Complex Scene Generation}} via {{Layout-guided Generative Gaussian Splatting}}},
  shorttitle = {{{GALA3D}}},
  author = {Zhou, Xiaoyu and Ran, Xingjian and Xiong, Yajiao and He, Jinlin and Lin, Zhiwei and Wang, Yongtao and Sun, Deqing and Yang, Ming-Hsuan},
  year = {2024},
  eprint = {2402.07207},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2402.07207},
  urldate = {2024-02-22},
  abstract = {We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an object-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. Source codes and models will be available at https://gala3d.github.io/.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/xuanenyun/Zotero/storage/S6ETSFED/Zhou ç­‰ - 2024 - GALA3D Towards Text-to-3D Complex Scene Generatio.pdf}
}
